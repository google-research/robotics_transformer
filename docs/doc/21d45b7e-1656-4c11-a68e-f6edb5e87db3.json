{
    "summary": "This code includes a SequenceAgent class for outputting actions and an RL agent class with methods for initializing the actor network, training, and calculating loss. It handles gradients, adds summaries, and returns LossInfo.",
    "details": [
        {
            "comment": "This code file contains a sequence policy and agent that directly output actions via an actor network. These classes are generic and not intended to change. The file imports necessary libraries for TensorFlow Agents, including data converter, TF agent, and networks.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/sequence_agent.py\":0-25",
            "content": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sequence policy and agent that directly output actions via actor network.\nThese classes are not intended to change as they are generic enough for any\nall-neural actor based agent+policy. All new features are intended to be\nimplemented in `actor_network` and `loss_fn`.\n\"\"\"\nfrom typing import Optional, Type\nfrom absl import logging\nimport tensorflow as tf\nfrom tf_agents.agents import data_converter\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.networks import network"
        },
        {
            "comment": "The code defines a class `SequencePolicy` that inherits from `actor_policy.ActorPolicy`. It sets the actions, gets actor loss and auxiliary information, sets training mode, and applies an actor network to get actions for given time steps and policy states.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/sequence_agent.py\":26-59",
            "content": "from tf_agents.policies import actor_policy\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\nclass SequencePolicy(actor_policy.ActorPolicy):\n  \"\"\"A policy that directly outputs actions via an actor network.\"\"\"\n  def __init__(self, **kwargs):\n    self._actions = None\n    super().__init__(**kwargs)\n  def set_actions(self, actions):\n    self._actor_network.set_actions(actions)\n  def get_actor_loss(self):\n    return self._actor_network.get_actor_loss()\n  def get_aux_info(self):\n    return self._actor_network.get_aux_info()\n  def set_training(self, training):\n    self._training = training\n  def _action(self,\n              time_step: ts.TimeStep,\n              policy_state: types.NestedTensor,\n              seed: Optional[types.Seed] = None) -> policy_step.PolicyStep:\n    del seed\n    action, policy_state = self._apply_actor_network(\n        time_step.observation,\n        step_type=time_step.step_type,"
        },
        {
            "comment": "This code defines a SequenceAgent class that extends the tf_agent.TFAgent class and includes an actor network for generating actions. The agent takes time step and policy state as inputs and returns action, policy state, and info. It also has an _info_spec attribute to specify the information returned by the policy step.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/sequence_agent.py\":60-85",
            "content": "        policy_state=policy_state)\n    info = ()\n    return policy_step.PolicyStep(action, policy_state, info)\n  def _distribution(self, time_step, policy_state):\n    current_step = super()._distribution(time_step, policy_state)\n    return current_step\nclass SequenceAgent(tf_agent.TFAgent):\n  \"\"\"A sequence agent that directly outputs actions via an actor network.\"\"\"\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedTensorSpec,\n               actor_network: Type[network.Network],\n               actor_optimizer: tf.keras.optimizers.Optimizer,\n               policy_cls: Type[actor_policy.ActorPolicy] = SequencePolicy,\n               time_sequence_length: int = 6,\n               debug_summaries: bool = False,\n               **kwargs):\n    self._info_spec = ()\n    self._actor_network = actor_network(  # pytype: disable=missing-parameter  # dynamic-method-lookup\n        input_tensor_spec=time_step_spec.observation,\n        output_tensor_spec=action_spec,\n        policy_info_spec=self._info_spec,"
        },
        {
            "comment": "The code defines a SequenceAgent class with train and collect policies, initializes the actor optimizer, and sets up the data context for time steps, actions, and information specification. The collect policy is used as the eval policy as well.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/sequence_agent.py\":86-113",
            "content": "        train_step_counter=kwargs['train_step_counter'],\n        time_sequence_length=time_sequence_length)\n    self._actor_optimizer = actor_optimizer\n    # Train policy is only used for loss and never exported as saved_model.\n    self._train_policy = policy_cls(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        info_spec=self._info_spec,\n        actor_network=self._actor_network,\n        training=True)\n    collect_policy = policy_cls(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        info_spec=self._info_spec,\n        actor_network=self._actor_network,\n        training=False)\n    super(SequenceAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        collect_policy,  # We use the collect_policy as the eval policy.\n        collect_policy,\n        train_sequence_length=time_sequence_length,\n        **kwargs)\n    self._data_context = data_converter.DataContext(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        info_spec=collect_policy.info_spec,"
        },
        {
            "comment": "The code defines a class with methods for initializing the actor network, training the agent using experience and weights, and applying gradients. It also prints the number of parameters in the actor network.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/sequence_agent.py\":114-139",
            "content": "        use_half_transition=True)\n    self.as_transition = data_converter.AsHalfTransition(\n        self._data_context, squeeze_time_dim=False)\n    self._debug_summaries = debug_summaries\n    num_params = 0\n    for weight in self._actor_network.trainable_weights:\n      weight_params = 1\n      for dim in weight.shape:\n        weight_params *= dim\n      logging.info('%s has %s params.', weight.name, weight_params)\n      num_params += weight_params\n    logging.info('Actor network has %sM params.', round(num_params / 1000000.,\n                                                        2))\n  def _train(self, experience: types.NestedTensor,\n             weights: types.Tensor) -> tf_agent.LossInfo:\n    self.train_step_counter.assign_add(1)\n    loss_info = self._loss(experience, weights, training=True)\n    self._apply_gradients(loss_info.loss)\n    return loss_info\n  def _apply_gradients(self, loss: types.Tensor):\n    variables = self._actor_network.trainable_weights\n    gradients = tf.gradients(loss, variables)\n    # Skip nan and inf gradients."
        },
        {
            "comment": "This code snippet is part of a reinforcement learning agent. It handles gradients and applies them to variables using an optimizer. The '_loss' function calculates the loss for the policy in training mode, taking into account actions, time steps, and valid masking.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/sequence_agent.py\":140-163",
            "content": "    new_gradients = []\n    for g in gradients:\n      if g is not None:\n        new_g = tf.where(\n            tf.math.logical_or(tf.math.is_inf(g), tf.math.is_nan(g)),\n            tf.zeros_like(g), g)\n        new_gradients.append(new_g)\n      else:\n        new_gradients.append(g)\n    grads_and_vars = list(zip(new_gradients, variables))\n    self._actor_optimizer.apply_gradients(grads_and_vars)\n  def _loss(self, experience: types.NestedTensor, weights: types.Tensor,\n            training: bool) -> tf_agent.LossInfo:\n    transition = self.as_transition(experience)\n    time_steps, policy_steps, _ = transition\n    batch_size = nest_utils.get_outer_shape(time_steps, self._time_step_spec)[0]\n    policy = self._train_policy\n    policy.set_actions(policy_steps.action)\n    policy.set_training(training=training)\n    with tf.name_scope('actor_loss'):\n      policy_state = policy.get_initial_state(batch_size)\n      policy.action(time_steps, policy_state=policy_state)\n      valid_mask = tf.cast(~time_steps.is_last(), tf.float32)"
        },
        {
            "comment": "The code calculates the actor loss by multiplying it with the valid mask, then reduces the mean of the loss. It sets the actions to None and adds summaries for the observations using the actor network's auxiliary information. Finally, it returns a LossInfo object containing the loss and extra information (which is just the loss in this case).",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/sequence_agent.py\":164-170",
            "content": "      loss = valid_mask * policy.get_actor_loss()\n      loss = tf.reduce_mean(loss)\n      policy.set_actions(None)\n      self._actor_network.add_summaries(time_steps.observation,\n                                        policy.get_aux_info(),\n                                        self._debug_summaries, training)\n      return tf_agent.LossInfo(loss=loss, extra=loss)"
        }
    ]
}