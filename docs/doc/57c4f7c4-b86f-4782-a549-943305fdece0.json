{
    "summary": "This code tests the ActionTokenizer module in Python, verifying action tokenization and detokenization accuracy, including handling of episode termination actions, int32 values, and ensuring correct object creation.",
    "details": [
        {
            "comment": "This code is for testing the ActionTokenizer module in Python, which is used to tokenize actions in a robotics transformer model. The test checks the tokenization functionality when dealing with int32 action specifications and ensures that the episode termination action is correctly handled. The code imports necessary libraries, defines the required classes for the test, and sets up the test case.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/action_tokenizer_test.py\":0-26",
            "content": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for action_tokenizer.\"\"\"\nimport numpy as np\nfrom robotics_transformer.tokenizers import action_tokenizer\nfrom tensor2robot.utils import tensorspec_utils\nimport tensorflow as tf\nfrom tf_agents.specs import tensor_spec\nclass ActionTokenizerTest(tf.test.TestCase):\n  def testTokenize_int32(self):\n    action_spec = tensorspec_utils.TensorSpecStruct()\n    action_spec.terminate_episode = tensor_spec.BoundedTensorSpec(\n        (2,), dtype=tf.int32, minimum=0, maximum=1, name='terminate_episode')"
        },
        {
            "comment": "The code tests the functionality of the RT1ActionTokenizer in various scenarios. The first test checks if tokenizing an action with one token per episode returns a numpy array containing 1. The second test checks if tokenizing an action with non-one-hot int32 values raises a tf.errors.InvalidArgumentError exception. The third test checks the detokenization process for actions encoded as int32 values.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/action_tokenizer_test.py\":27-46",
            "content": "    tokenizer = action_tokenizer.RT1ActionTokenizer(action_spec, vocab_size=10)\n    self.assertEqual(1, tokenizer.tokens_per_action)\n    action = tensorspec_utils.TensorSpecStruct(terminate_episode=[0, 1])\n    action_tokens = tokenizer.tokenize(action)\n    self.assertEqual([1], action_tokens.numpy())\n  def testTokenize_int32_not_one_hot(self):\n    action_spec = tensorspec_utils.TensorSpecStruct()\n    action_spec.terminate_episode = tensor_spec.BoundedTensorSpec(\n        (2,), dtype=tf.int32, minimum=0, maximum=1, name='terminate_episode')\n    tokenizer = action_tokenizer.RT1ActionTokenizer(action_spec, vocab_size=10)\n    self.assertEqual(1, tokenizer.tokens_per_action)\n    action = tensorspec_utils.TensorSpecStruct(terminate_episode=[1, 8])\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      tokenizer.tokenize(action)\n  def testDetokenize_int32(self):\n    action_spec = tensorspec_utils.TensorSpecStruct()\n    action_spec.terminate_episode = tensor_spec.BoundedTensorSpec(\n        (2,), dtype=tf.int32, minimum=0, maximum=1, name='terminate_episode')"
        },
        {
            "comment": "The code tests the action_tokenizer by detokenizing various input actions. For 0 token, it should become [1, 0]. For 1 token, it should become [0, 1]. For an OOV (out-of-vocabulary) 3 token, it should become [1, 0]. The code also tests the action_tokenizer with float input for a bounded tensor spec. It confirms that tokens_per_action is correctly set to 3.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/action_tokenizer_test.py\":47-63",
            "content": "    tokenizer = action_tokenizer.RT1ActionTokenizer(action_spec, vocab_size=10)\n    # 0 token should become a one hot: [1, 0]\n    action = tokenizer.detokenize(tf.constant([0], dtype=tf.int32))\n    self.assertSequenceEqual([1, 0], list(action['terminate_episode'].numpy()))\n    # 1 token should become a one hot: [0, 1]\n    action = tokenizer.detokenize(tf.constant([1], dtype=tf.int32))\n    self.assertSequenceEqual([0, 1], list(action['terminate_episode'].numpy()))\n    # OOV 3 token should become a default one hot: [1, 0]\n    action = tokenizer.detokenize(tf.constant([3], dtype=tf.int32))\n    self.assertSequenceEqual([1, 0], list(action['terminate_episode'].numpy()))\n  def testTokenize_float(self):\n    action_spec = tensorspec_utils.TensorSpecStruct()\n    action_spec.world_vector = tensor_spec.BoundedTensorSpec(\n        (3,), dtype=tf.float32, minimum=-1., maximum=1., name='world_vector')\n    tokenizer = action_tokenizer.RT1ActionTokenizer(action_spec, vocab_size=10)\n    self.assertEqual(3, tokenizer.tokens_per_action)"
        },
        {
            "comment": "In this code, an action tokenizer is being tested. The action_spec defines a tensor with 3 dimensions and bounded values between -1 and 1. The RT1ActionTokenizer is initialized with this action_spec and vocab_size=10. It checks that the tokens_per_action is 3. Then, it creates an action tensor of size [batch_size, time_dimension, tokens_per_action] and tokenizes it using the tokenizer. The code asserts that the resulting action_tokens have the correct shape [batch_size, time_dimension, tokens_per_action].",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/action_tokenizer_test.py\":64-83",
            "content": "    action = tensorspec_utils.TensorSpecStruct(world_vector=[0.1, 0.5, -0.8])\n    action_tokens = tokenizer.tokenize(action)\n    self.assertSequenceEqual([4, 6, 0], list(action_tokens.numpy()))\n  def testTokenize_float_with_time_dimension(self):\n    action_spec = tensorspec_utils.TensorSpecStruct()\n    action_spec.world_vector = tensor_spec.BoundedTensorSpec(\n        (3,), dtype=tf.float32, minimum=-1., maximum=1., name='world_vector')\n    tokenizer = action_tokenizer.RT1ActionTokenizer(action_spec, vocab_size=10)\n    self.assertEqual(3, tokenizer.tokens_per_action)\n    batch_size = 2\n    time_dimension = 3\n    action = tensorspec_utils.TensorSpecStruct(\n        world_vector=tf.constant(\n            [[0.1, 0.5, -0.8], [0.1, 0.5, -0.8], [0.1, 0.5, -0.8],\n             [0.1, 0.5, -0.8], [0.1, 0.5, -0.8], [0.1, 0.5, -0.8]],\n            shape=[batch_size, time_dimension, tokenizer.tokens_per_action]))\n    action_tokens = tokenizer.tokenize(action)\n    self.assertSequenceEqual(\n        [batch_size, time_dimension, tokenizer.tokens_per_action],"
        },
        {
            "comment": "The code defines two functions `testTokenize_float_at_limits` and `testTokenize_invalid_action_spec_shape`. The first function tests the tokenization of actions with float values at their limits, ensuring that minimum value maps to 0 and maximum value maps to vocab_size-1. The second function tests tokenizing an action with an invalid shape for its action specification.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/action_tokenizer_test.py\":84-109",
            "content": "        action_tokens.shape.as_list())\n  def testTokenize_float_at_limits(self):\n    minimum = -1.\n    maximum = 1.\n    vocab_size = 10\n    action_spec = tensorspec_utils.TensorSpecStruct()\n    action_spec.world_vector = tensor_spec.BoundedTensorSpec(\n        (2,),\n        dtype=tf.float32,\n        minimum=minimum,\n        maximum=maximum,\n        name='world_vector')\n    tokenizer = action_tokenizer.RT1ActionTokenizer(\n        action_spec, vocab_size=vocab_size)\n    self.assertEqual(2, tokenizer.tokens_per_action)\n    action = tensorspec_utils.TensorSpecStruct(world_vector=[minimum, maximum])\n    action_tokens = tokenizer.tokenize(action)\n    # Minimum value will go to 0\n    # Maximum value witll go to vocab_size-1\n    self.assertSequenceEqual([0, vocab_size - 1], list(action_tokens.numpy()))\n  def testTokenize_invalid_action_spec_shape(self):\n    action_spec = tensorspec_utils.TensorSpecStruct()\n    action_spec.world_vector = tensor_spec.BoundedTensorSpec(\n        (2, 2), dtype=tf.float32, minimum=1, maximum=-1, name='world_vector')"
        },
        {
            "comment": "The code is testing the action tokenizer's tokenize and detokenize methods to ensure they are equal. The code creates an action specification for a robotics task, including various bounded tensor specifications such as world vector, rotation delta, gripper closedness action, terminate episode, etc., and tests the tokenizer with a vocabulary size of 10. It also checks if there is a ValueError raised when creating an RT1ActionTokenizer object with specified action_spec and vocab_size.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/action_tokenizer_test.py\":110-142",
            "content": "    with self.assertRaises(ValueError):\n      action_tokenizer.RT1ActionTokenizer(action_spec, vocab_size=10)\n  def testTokenizeAndDetokenizeIsEqual(self):\n    action_spec = tensorspec_utils.TensorSpecStruct()\n    action_spec.world_vector = tensor_spec.BoundedTensorSpec(\n        (3,), dtype=tf.float32, minimum=-1., maximum=1., name='world_vector')\n    action_spec.rotation_delta = tensor_spec.BoundedTensorSpec(\n        (3,),\n        dtype=tf.float32,\n        minimum=-np.pi / 2.,\n        maximum=np.pi / 2.,\n        name='rotation_delta')\n    action_spec.gripper_closedness_action = tensor_spec.BoundedTensorSpec(\n        (1,),\n        dtype=tf.float32,\n        minimum=-1.,\n        maximum=1.,\n        name='gripper_closedness_action')\n    num_sub_action_space = 2\n    action_spec.terminate_episode = tensor_spec.BoundedTensorSpec(\n        (num_sub_action_space,),\n        dtype=tf.int32,\n        minimum=0,\n        maximum=1,\n        name='terminate_episode')\n    tokenizer = action_tokenizer.RT1ActionTokenizer(\n        action_spec,"
        },
        {
            "comment": "This code tests the tokenizer class in the action_tokenizer_test.py file, ensuring it correctly tokenizes and detokenizes actions. It performs these tests 10 times with randomized inputs for better accuracy. The code asserts that the tokenized and detokenized values are nearly equal to the original action values, checking each component separately.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/action_tokenizer_test.py\":143-169",
            "content": "        vocab_size=1024,\n        action_order=[\n            'terminate_episode', 'world_vector', 'rotation_delta',\n            'gripper_closedness_action'\n        ])\n    self.assertEqual(8, tokenizer.tokens_per_action)\n    # Repeat the following test N times with fuzzy inputs.\n    n_repeat = 10\n    for _ in range(n_repeat):\n      action = tensorspec_utils.TensorSpecStruct(\n          world_vector=np.random.uniform(low=-1., high=1.0, size=3),\n          rotation_delta=np.random.uniform(\n              low=-np.pi / 2., high=np.pi / 2., size=3),\n          gripper_closedness_action=np.random.uniform(low=0., high=1.0, size=1),\n          terminate_episode=[0, 1])\n      action_tokens = tokenizer.tokenize(action)\n      policy_action = tokenizer.detokenize(action_tokens)\n      for k in action:\n        self.assertSequenceAlmostEqual(\n            action[k], policy_action[k].numpy(), places=2)\n      # Repeat the test with batched actions\n      batched_action = tensorspec_utils.TensorSpecStruct(\n          world_vector=[\n              np.random.uniform(low=-1., high=1.0, size=3),"
        },
        {
            "comment": "This code is initializing a random action for a robotics transformer tokenizer test. It generates three random translation values, two rotation deltas, and two gripper closedness actions. The tokenizer then tokenizes the batched action and detokenizes it to create policy_action. Finally, the code compares the original action with the detokenized policy_action using assertSequenceAlmostEqual function.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/action_tokenizer_test.py\":170-190",
            "content": "              np.random.uniform(low=-1., high=1.0, size=3)\n          ],\n          rotation_delta=[\n              np.random.uniform(low=-np.pi / 2., high=np.pi / 2., size=3),\n              np.random.uniform(low=-np.pi / 2., high=np.pi / 2., size=3)\n          ],\n          gripper_closedness_action=[\n              np.random.uniform(low=0., high=1.0, size=1),\n              np.random.uniform(low=0., high=1.0, size=1)\n          ],\n          terminate_episode=[[0, 1], [1, 0]])\n      action_tokens = tokenizer.tokenize(batched_action)\n      policy_action = tokenizer.detokenize(action_tokens)\n      for k in batched_action:\n        for a, policy_a in zip(batched_action[k], policy_action[k].numpy()):\n          self.assertSequenceAlmostEqual(a, policy_a, places=2)\nif __name__ == '__main__':\n  tf.test.main()"
        }
    ]
}