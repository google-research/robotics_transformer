{
    "summary": "The code defines a Transformer layer with configurable parameters and a Multi-Head Attention layer for processing input tensors. The transformer model applies attention and embeddings, taking an input tensor x and outputs attention scores.",
    "details": [
        {
            "comment": "This code defines a Transformer layer, which is a single transformer block. It has configurable parameters such as layer size, number of heads, feed-forward size, dropout rate, and returns attention scores optionally.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer.py\":0-31",
            "content": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"RT1 decoder transformer.\nCopied from:\nhttps://www.tensorflow.org/text/tutorials/transformer#decoder\n\"\"\"\nfrom typing import Tuple, Union\nimport tensorflow as tf\nclass _TransformerLayer(tf.keras.layers.Layer):\n  \"\"\"A single transformer block.\"\"\"\n  def __init__(self,\n               layer_size: int = 4096,\n               num_heads: int = 8,\n               feed_forward_size: int = 512,\n               dropout_rate: float = 0.1,\n               return_attention_scores: bool = False):"
        },
        {
            "comment": "The code defines a Transformer layer with multiple head attention, feed-forward layer, and layer normalization. It takes arguments such as layer size, number of heads, feed-forward size, dropout rate, and whether to return attention scores. The call function is responsible for processing the input tensor, applying multi-head attention, feed-forward layers, and layer normalization.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer.py\":32-56",
            "content": "    \"\"\"Creates a Transformer layer.\n    Args:\n      layer_size: Size of the multiple head attention layer.\n      num_heads: Number of heads for the multiple head attention layer.\n      feed_forward_size: Dimensionality of the feed_forward layer.\n      dropout_rate: Dropout rate.\n      return_attention_scores: Return attention scores.\n    \"\"\"\n    super(_TransformerLayer, self).__init__()\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.mha1 = tf.keras.layers.MultiHeadAttention(\n        key_dim=layer_size, num_heads=num_heads, dropout=dropout_rate)\n    self.ff = tf.keras.layers.Dense(feed_forward_size)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.dropout_ff = tf.keras.layers.Dropout(dropout_rate)\n    self._return_attention_scores = return_attention_scores\n  def call(self, x: tf.Tensor, attention_mask: tf.Tensor,\n           training: bool) -> Tuple[tf.Tensor, Union[tf.Tensor, None]]:\n    \"\"\"Calls the layer.\n    Args:\n      x: Input Tensor of shape `(B, T, dim)`."
        },
        {
            "comment": "The code defines a Multi-Head Attention layer for the transformer. It takes input `x`, applies layer normalization, passes it to the MHA block, adds it back to the original input, and then applies another layer normalization before returning the output as `y`. The attention scores are optional based on the `_return_attention_scores` flag.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer.py\":57-84",
            "content": "      attention_mask: a boolean mask of shape `(B, T, T)`, that prevents\n        attention to certain positions. The boolean mask specifies which query\n        elements can attend to which key elements, 1 indicates attention and 0\n        indicates no attention. Broadcasting can happen for the missing batch\n        dimensions and the head dimension.\n      training: Python boolean indicating whether the layer should behave in\n        training mode (adding dropout) or in inference mode (no dropout).\n    Returns:\n      y: Output Tensor of shape `(B, T, dim)`. Also return the attention scores\n      of shape `(B, T, dim)` or None.\n    \"\"\"\n    x1 = self.layernorm1(x)\n    mha_results = self.mha1(\n        query=x1,\n        key=x1,\n        value=x1,\n        attention_mask=attention_mask,\n        return_attention_scores=self._return_attention_scores,\n        training=training)\n    if self._return_attention_scores:\n      x1, score = mha_results\n    else:\n      x1, score = mha_results, None\n    x = x + x1\n    y = self.layernorm2(x)"
        },
        {
            "comment": "This code snippet defines a decoder only transformer, which is a type of neural network layer. The Transformer class takes in several parameters such as number of layers, size of the multiple head attention layer, number of heads, dimensionality of feed_forward layer, dropout rate, and dimensionality of tokens from output layer. It also has an optional argument to return attention scores or not. The class initializes a list of TransformerLayer objects using a list comprehension to create each layer with the specified parameters.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer.py\":85-116",
            "content": "    ff_y = self.ff(y)\n    ff_y = self.dropout_ff(ff_y, training=training)\n    x = x + ff_y\n    return x, score\nclass Transformer(tf.keras.layers.Layer):\n  \"\"\"A decoder only transformer.\"\"\"\n  def __init__(self,\n               num_layers: int = 1,\n               layer_size: int = 4096,\n               num_heads: int = 8,\n               feed_forward_size: int = 512,\n               dropout_rate: float = 0.1,\n               vocab_size: int = 256,\n               return_attention_scores: bool = False):\n    \"\"\"Creates a transformer.\n    Args:\n      num_layers: Number of transformer layers.\n      layer_size: Size of the multiple head attention layer.\n      num_heads: Number of heads for the multiple head attention layer.\n      feed_forward_size: Dimensionality of the feed_forward layer.\n      dropout_rate: Dropout rate.\n      vocab_size: Dimensionality of tokens from the output layer.\n      return_attention_scores: Return attention scores.\n    \"\"\"\n    super(Transformer, self).__init__()\n    self._layers = [\n        _TransformerLayer(  # pylint: disable=g-complex-comprehension"
        },
        {
            "comment": "This code initializes a transformer model with specified parameters and defines its call function. The model consists of multiple layers, each with a specific layer size, number of heads, feed-forward size, dropout rate, and attention score return option. The model also includes dense layers for token and position embeddings, as well as an output dense layer. The call function takes input tensors, training state, and an attention mask, and returns either a tensor or a tuple of tensors (including attention scores) based on the model's behavior during training or inference.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer.py\":117-142",
            "content": "            layer_size=layer_size,\n            num_heads=num_heads,\n            feed_forward_size=feed_forward_size,\n            dropout_rate=dropout_rate,\n            return_attention_scores=return_attention_scores)\n        for _ in range(num_layers)\n    ]\n    self._token_emb = tf.keras.layers.Dense(feed_forward_size)\n    self._position_emb = tf.keras.layers.Dense(feed_forward_size)\n    self._output_tokens = tf.keras.layers.Dense(vocab_size)\n  def call(\n      self,\n      x: tf.Tensor,\n      training: bool,\n      attention_mask: tf.Tensor,\n  ) -> Union[tf.Tensor, Tuple[tf.Tensor, list[tf.Tensor]]]:\n    \"\"\"Calls the layer.\n    Args:\n      x: Input Tensor of shape `(B, T, dim)`.\n      training: Python boolean indicating whether the layer should behave in\n        training mode (adding dropout) or in inference mode (no dropout).\n      attention_mask: a boolean mask of shape `(B, T, T)`, that prevents\n        attention to certain positions. The boolean mask specifies which query\n        elements can attend to which key elements, 1 indicates attention and 0"
        },
        {
            "comment": "This code implements a transformer model, performing attention mechanisms and token embeddings. It takes an input tensor x of shape `(B, T, vocab_size)` where B is batch size, T is sequence length, and vocab_size is the number of tokens in the vocabulary. The model applies position and token embeddings, then passes through layers of the transformer model, which outputs attention scores. The output tensor x has shape `(B, T, vocab_size)` and scores are returned as a list for each layer in the model.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer.py\":143-168",
            "content": "        indicates no attention. Broadcasting can happen for the missing batch\n        dimensions and the head dimension.\n    Returns:\n      x: Output Tensor of shape `(B, T, vocab_size)`. If\n      `return_attention_scores`, also return attention scores of\n      a list of `layer` of elements with shape `(B, T, dim)`.\n    \"\"\"\n    seq_len = tf.shape(x)[1]\n    batch_size = tf.shape(x)[0]\n    positions = tf.one_hot(\n        tf.tile(tf.expand_dims(tf.range(0, seq_len, 1), 0), [batch_size, 1]),\n        seq_len)\n    x = self._token_emb(x)\n    x += self._position_emb(positions)\n    scores = []\n    for layer in self._layers:\n      x, score = layer(x, attention_mask=attention_mask, training=training)\n      if score is not None:\n        scores.append(score)\n    x = self._output_tokens(x)\n    return x, scores"
        }
    ]
}