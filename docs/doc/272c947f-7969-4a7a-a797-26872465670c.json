{
    "summary": "This code provides a TensorFlow implementation of the Token Learner for robotics_transformer project, with GELU activation and transformer MLP block. It initializes Transformer token learner layer using dense layers, defines TokenLearnerModule class, and performs token learning via MlpBlock and layer normalization before returning feature map with shape [bs, n_token, c].",
    "details": [
        {
            "comment": "This code snippet is a part of the \"robotics_transformer\" project and contains TensorFlow implementation of Token Learner as described in Ryoo et al 2021. It includes definitions for GELU activation function, and _maybe_dropout helper function that returns Dropout layer if rate is non-zero.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/token_learner.py\":0-29",
            "content": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"TF implementation of Token Learner(Ryoo et al 2021).\"\"\"\nimport functools\nfrom typing import Optional, Sequence, Union\nimport numpy as np\nimport tensorflow as tf\ndef gelu(x: float) -> float:\n  return 0.5 * x * (1 +\n                    tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\ndef _maybe_dropout(rate: float = 0.0, name: str = \"dropout\"):\n  \"\"\"Helper function to return dropout layer if rate is non zero.\"\"\"\n  if rate:\n    return tf.keras.layers.Dropout(rate, name=name)"
        },
        {
            "comment": "The code defines a transformer MLP block (Feed-Forward Block) that takes in parameters like mlp_dim, out_dim, kernel_init, bias_init, dropout_rate. It computes outer_dense(gelu(hidden_dense(input))) with dropout applied as necessary. Note that outside a keras workflow, layer.build should be called.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/token_learner.py\":30-57",
            "content": "  return lambda x, *args: x  # Does nothing to x.\nclass MlpBlock(tf.keras.layers.Layer):\n  \"\"\"Transformer MLP / feed-forward block.\"\"\"\n  def __init__(self,\n               *,\n               mlp_dim: int,\n               out_dim: Optional[int] = None,\n               kernel_init: Optional[tf.keras.initializers.Initializer] = tf\n               .keras.initializers.glorot_uniform(),\n               bias_init: Optional[tf.keras.initializers.Initializer] = tf.keras\n               .initializers.RandomNormal(stddev=1e-6),\n               dropout_rate: float = 0.1,\n               **kwargs):\n    \"\"\"Initializer for the MLP Block.\n    This computes outer_dense(gelu(hidden_dense(input))), with dropout\n    applied as necessary.\n    Note: Especially outside a keras workflow, make sure to call layer.build\n    Args:\n      mlp_dim: The dimension of the inner representation (output of hidden\n        layer). Usually larger than the input/output dim.\n      out_dim: The output dimension of the block. If None, the model output dim\n        is equal to the input dim (usually desired)"
        },
        {
            "comment": "This code initializes a Transformer token learner layer with optional arguments including the output dimension (out_dim), dropout rate, and other keyword arguments. It creates hidden and output layers using dense layers from tf.keras.layers. The hidden layer uses gelu activation function, while the output layer is initialized as a partial function of tf.keras.layers.Dense.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/token_learner.py\":58-83",
            "content": "      kernel_init: Initializer for dense kernels, used for both dense layers.\n      bias_init: Initializer for dense biases, used for both dense layers.\n      dropout_rate: Dropout rate to be applied after dense ( & activation)\n      **kwargs: Other keyword args passed to the tf.keras.layers.Layer\n        constructor e.g. the name\n    \"\"\"\n    super().__init__(**kwargs)\n    self._out_dim = out_dim\n    self._hidden_dropout = _maybe_dropout(dropout_rate)\n    self._output_dropout = _maybe_dropout(dropout_rate)\n    self._hidden_layer = tf.keras.layers.Dense(\n        mlp_dim,\n        activation=gelu,\n        kernel_initializer=kernel_init,\n        bias_initializer=bias_init,\n        name=\"hidden_dense\")\n    # If out_dim is None, infer out_dim = input_dim at self.build()\n    self._output_layer = functools.partial(\n        tf.keras.layers.Dense,\n        kernel_initializer=kernel_init,\n        bias_initializer=bias_init,\n        name=\"final_dense\")\n  def build(self, input_shape: Sequence[int]):\n    out_dim = self._out_dim or input_shape[-1]"
        },
        {
            "comment": "The code defines a TokenLearnerModule class that applies a Transformer MlpBlock and includes layer normalization. It takes the number of tokens, bottleneck dimension, and dropout rate as parameters. The call function performs token learning by first applying the MlpBlock and then the layer normalization.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/token_learner.py\":84-115",
            "content": "    self._output_layer = self._output_layer(units=out_dim)\n    super().build(input_shape)\n  def call(self,\n           inputs: tf.Tensor,\n           *,\n           is_training: Union[bool, tf.Tensor] = False) -> tf.Tensor:\n    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n    x = self._hidden_layer(inputs)\n    x = self._hidden_dropout(x, is_training)\n    x = self._output_layer(x)\n    x = self._output_dropout(x, is_training)\n    return x\nclass TokenLearnerModule(tf.keras.layers.Layer):\n  \"\"\"TokenLearner module V1.1 (https://arxiv.org/abs/2106.11297).\"\"\"\n  def __init__(self,\n               num_tokens: int,\n               bottleneck_dim: int = 64,\n               dropout_rate: float = 0.):\n    super().__init__()\n    self.mlp = MlpBlock(\n        mlp_dim=bottleneck_dim, out_dim=num_tokens, dropout_rate=dropout_rate)\n    self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n  def call(self, inputs: tf.Tensor, training: bool = False) -> tf.Tensor:\n    if len(inputs.shape) == 4:\n      bs, h, w, c = inputs.shape\n      inputs = tf.reshape(inputs, [bs, h * w, c])"
        },
        {
            "comment": "This code snippet is part of a token learning model. It performs layer normalization, applies multi-layer perceptron (MLP) for feature extraction, transposes the dimensions, applies softmax activation, and then performs feature extraction using Einstein summation. Finally, it returns the feature map with shape [bs, n_token, c].",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/tokenizers/token_learner.py\":117-127",
            "content": "    selected = self.layernorm(inputs)\n    selected = self.mlp(\n        selected, is_training=training)  # Shape: [bs, h*w, n_token].\n    selected = tf.transpose(selected, [0, 2, 1])  # Shape: [bs, n_token, h*w].\n    selected = tf.nn.softmax(selected, axis=-1)\n    feat = tf.einsum(\"...si,...id->...sd\", selected, inputs)\n    return feat  # Shape: [bs, n_token, c]"
        }
    ]
}