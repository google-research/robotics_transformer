{
    "summary": "The code configures testing parameters for robotics_transformer, sets up FakeImageTokenizer class and test data, defines constants, functions, and a transformer network using TensorFlow for image processing and robotics. It constructs a Transformer network for training and tests its functionality, ensuring alignment, verifying indexing, and updating output tokens.",
    "details": [
        {
            "comment": "Code block contains necessary import statements and variable definitions for tests related to networks in the robotics_transformer package. The code sets up parameters for testing, including batch size, time sequence length, and height dimensions.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":0-29",
            "content": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for networks.\"\"\"\nimport copy\nfrom typing import Optional, Tuple, Union\nfrom absl.testing import parameterized\nimport numpy as np\nfrom robotics_transformer import sequence_agent\nfrom robotics_transformer import transformer_network\nfrom tensor2robot.utils import tensorspec_utils\nimport tensorflow as tf\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nBATCH_SIZE = 2\nTIME_SEQUENCE_LENGTH = 3\nHEIGHT = 256"
        },
        {
            "comment": "This code defines constants WIDTH and NUM_IMAGE_TOKENS, and functions spec_names_list() and state_spec_list(). The former lists types of specs accepted by the transformer. The latter defines different types of state specs including image and natural language embedding.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":30-54",
            "content": "WIDTH = 320\nNUM_IMAGE_TOKENS = 2\ndef spec_names_list() -> list[str]:\n  \"\"\"Lists the different types of specs accepted by the transformer.\"\"\"\n  return ['default']\ndef state_spec_list() -> list[tensorspec_utils.TensorSpecStruct]:\n  \"\"\"Lists the different types of state spec accepted by the transformer.\"\"\"\n  state_spec = tensorspec_utils.TensorSpecStruct()\n  state_spec.image = tensor_spec.BoundedTensorSpec([HEIGHT, WIDTH, 3],\n                                                   dtype=tf.float32,\n                                                   name='image',\n                                                   minimum=0.,\n                                                   maximum=1.)\n  state_spec.natural_language_embedding = tensor_spec.TensorSpec(\n      shape=[512], dtype=tf.float32, name='natural_language_embedding')\n  state_spec_mask = copy.deepcopy(state_spec)\n  state_spec_mask.initial_binary_mask = tensor_spec.BoundedTensorSpec(\n      [HEIGHT, WIDTH, 1],\n      dtype=tf.int32,\n      name='initial_binary_mask',"
        },
        {
            "comment": "The function `setup_state_spec()` creates a state specification for the transformer network, including bounded and unbounded tensor specifications. The `observations_list()` function lists the different types of observations accepted by the transformer during training or inference. During training, it expects a batch size of 256 with additional dimensions for time sequence length and spatial image dimensions. In inference mode, only a single image is expected. These specifications are used to define the shape and type of input data for the network.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":55-88",
            "content": "      minimum=0,\n      maximum=255)\n  state_spec_tcl = copy.deepcopy(state_spec)\n  state_spec_tcl.original_image = tensor_spec.BoundedTensorSpec(\n      [HEIGHT, WIDTH, 3],\n      dtype=tf.float32,\n      name='original_image',\n      minimum=0.,\n      maximum=1.)\n  return [\n      state_spec,\n      state_spec_mask,\n      state_spec_tcl,\n  ]\ndef observations_list(training: bool = True) -> list[dict[str, tf.Tensor]]:\n  \"\"\"Lists the different types of observations accepted by the transformer.\"\"\"\n  if training:\n    image_shape = [BATCH_SIZE, TIME_SEQUENCE_LENGTH, HEIGHT, WIDTH, 3]\n    emb_shape = [BATCH_SIZE, TIME_SEQUENCE_LENGTH, 512]\n    mask_shape = [BATCH_SIZE, TIME_SEQUENCE_LENGTH, HEIGHT, WIDTH, 1]\n  else:\n    # inference currently only support batch size of 1\n    image_shape = [1, HEIGHT, WIDTH, 3]\n    emb_shape = [1, 512]\n    mask_shape = [1, HEIGHT, WIDTH, 1]\n  return [\n      {\n          'image': tf.constant(0.5, shape=image_shape),\n          'natural_language_embedding': tf.constant(1., shape=emb_shape),\n      },"
        },
        {
            "comment": "This code snippet defines a FakeImageTokenizer class for testing Transformer models. It also sets up various test data dictionaries, including initial state specifications and observations. The code includes constants for image shape, embedding shape, and mask shape. It also uses TensorFlow constants to create fake image and natural language embedding data.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":89-117",
            "content": "      {\n          'image': tf.constant(0.5, shape=image_shape),\n          'natural_language_embedding': tf.constant(1., shape=emb_shape),\n          'initial_binary_mask': tf.constant(192, shape=mask_shape),\n      },\n      {  # This is used for TCL.\n          'image': tf.constant(0.5, shape=image_shape),\n          'original_image': tf.constant(0.4, shape=image_shape),\n          'natural_language_embedding': tf.constant(1., shape=emb_shape),\n      },\n  ]\nNAME_TO_STATE_SPECS = dict(zip(spec_names_list(), state_spec_list()))\nNAME_TO_OBSERVATIONS = dict(zip(spec_names_list(), observations_list()))\nNAME_TO_INF_OBSERVATIONS = dict(\n    zip(spec_names_list(), observations_list(False)))\nclass FakeImageTokenizer(tf.keras.layers.Layer):\n  \"\"\"Fake Image Tokenizer for testing Transformer.\"\"\"\n  def __init__(self,\n               encoder: ...,\n               position_embedding: ...,\n               embedding_output_dim: int,\n               patch_size: int,\n               use_token_learner: bool = False,\n               num_tokens: int = NUM_IMAGE_TOKENS,"
        },
        {
            "comment": "This code is setting up a transformer network for image processing. It takes an image and optionally a context tensor as input, and outputs a transformed tensor. The number of tokens used per context image is determined by the `tokens_per_context_image` attribute. If `use_initial_binary_mask` is set to True, one additional token will be used. Initial binary mask should not be None if using initial binary mask. The code then loops through each sequence in the batch and creates a tensor of tokens for each sequence. Each token is initialized with a value from the first image in the batch at the corresponding time step.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":118-142",
            "content": "               use_initial_binary_mask: bool = False,\n               **kwargs):\n    del encoder, position_embedding, patch_size, use_token_learner\n    super().__init__(**kwargs)\n    self.tokens_per_context_image = num_tokens\n    if use_initial_binary_mask:\n      self.tokens_per_context_image += 1\n    self.embedding_output_dim = embedding_output_dim\n    self.use_initial_binary_mask = use_initial_binary_mask\n  def __call__(self,\n               image: tf.Tensor,\n               context: Optional[tf.Tensor] = None,\n               initial_binary_mask: Optional[tf.Tensor] = None,\n               training: bool = False) -> tf.Tensor:\n    if self.use_initial_binary_mask:\n      assert initial_binary_mask is not None\n    image_shape = tf.shape(image)\n    seq_size = image_shape[1]\n    batch_size = image_shape[0]\n    all_tokens = []\n    num_tokens = self.tokens_per_context_image\n    for t in range(seq_size):\n      tokens = tf.ones([batch_size, 1, num_tokens, self.embedding_output_dim\n                       ]) * image[0][t][0][0]"
        },
        {
            "comment": "This code defines a class named TransformerNetworkTestUtils that extends tf.test.TestCase and parameterized.TestCase. It also includes a method named _define_specs which defines specs, observations for training and inference, batch sizes, sequence lengths, token embedding size, and an action spec. The purpose of this code is to set up test specifications and utilities for the TransformerNetworkTest class.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":143-165",
            "content": "      all_tokens.append(tokens)\n    return tf.concat(all_tokens, axis=1)\nclass TransformerNetworkTestUtils(tf.test.TestCase, parameterized.TestCase):\n  \"\"\"Defines specs, SequenceAgent, and various other testing utilities.\"\"\"\n  def _define_specs(self,\n                    train_batch_size=BATCH_SIZE,\n                    inference_batch_size=1,\n                    time_sequence_length=TIME_SEQUENCE_LENGTH,\n                    inference_sequence_length=TIME_SEQUENCE_LENGTH,\n                    token_embedding_size=512,\n                    image_width=WIDTH,\n                    image_height=HEIGHT):\n    \"\"\"Defines specs and observations (both training and inference).\"\"\"\n    self.train_batch_size = train_batch_size\n    self.inference_batch_size = inference_batch_size\n    self.time_sequence_length = time_sequence_length\n    self.inference_sequence_length = inference_sequence_length\n    self.token_embedding_size = token_embedding_size\n    action_spec = tensorspec_utils.TensorSpecStruct()\n    action_spec.world_vector = tensor_spec.BoundedTensorSpec("
        },
        {
            "comment": "This code sets up the action and state specifications for a robotics transformer network. The action_spec includes specifications for world vector, rotation delta, gripper closedness action, and terminate episode actions. The state_spec includes specifications for image and natural language embedding. These specifications define the data types, shapes, and boundaries of the input data for both actions and states.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":166-195",
            "content": "        (3,), dtype=tf.float32, minimum=-1., maximum=1., name='world_vector')\n    action_spec.rotation_delta = tensor_spec.BoundedTensorSpec(\n        (3,),\n        dtype=tf.float32,\n        minimum=-np.pi / 2,\n        maximum=np.pi / 2,\n        name='rotation_delta')\n    action_spec.gripper_closedness_action = tensor_spec.BoundedTensorSpec(\n        (1,),\n        dtype=tf.float32,\n        minimum=-1.,\n        maximum=1.,\n        name='gripper_closedness_action')\n    action_spec.terminate_episode = tensor_spec.BoundedTensorSpec(\n        (2,), dtype=tf.int32, minimum=0, maximum=1, name='terminate_episode')\n    state_spec = tensorspec_utils.TensorSpecStruct()\n    state_spec.image = tensor_spec.BoundedTensorSpec(\n        [image_height, image_width, 3],\n        dtype=tf.float32,\n        name='image',\n        minimum=0.,\n        maximum=1.)\n    state_spec.natural_language_embedding = tensor_spec.TensorSpec(\n        shape=[self.token_embedding_size],\n        dtype=tf.float32,\n        name='natural_language_embedding')\n    self._policy_info_spec = {"
        },
        {
            "comment": "This code defines the data types and specifications for a robotics transformer network. It includes 'return', 'discounted_return' as bounded tensor specs with float32 datatype, and minimum and maximum values set to 0.0 and 1.0 respectively. The state and action specifications are also defined. An inference observation is created with a constant image shape and natural language embedding shape for inference batch size and token embedding size respectively.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":196-222",
            "content": "        'return':\n            tensor_spec.BoundedTensorSpec((),\n                                          dtype=tf.float32,\n                                          minimum=0.0,\n                                          maximum=1.0,\n                                          name='return'),\n        'discounted_return':\n            tensor_spec.BoundedTensorSpec((),\n                                          dtype=tf.float32,\n                                          minimum=0.0,\n                                          maximum=1.0,\n                                          name='discounted_return'),\n    }\n    self._state_spec = state_spec\n    self._action_spec = action_spec\n    self._inference_observation = {\n        'image':\n            tf.constant(\n                1,\n                shape=[self.inference_batch_size, image_height, image_width, 3],\n                dtype=tf.dtypes.float32),\n        'natural_language_embedding':\n            tf.constant(\n                1.,\n                shape=[self.inference_batch_size, self.token_embedding_size],"
        },
        {
            "comment": "This code sets up test data for a robotics transformer network. It defines constant values for train and inference actions, such as image, natural language embedding, world vector, rotation delta, terminate episode, and gripper closedness action, with shapes determined by batch size, time sequence length, and dimensions.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":223-251",
            "content": "                dtype=tf.dtypes.float32),\n    }\n    self._train_observation = {\n        'image':\n            tf.constant(\n                0.5,\n                shape=[\n                    self.train_batch_size, self.time_sequence_length,\n                    image_height, image_width, 3\n                ]),\n        'natural_language_embedding':\n            tf.constant(\n                1.,\n                shape=[\n                    self.train_batch_size, self.time_sequence_length,\n                    self.token_embedding_size\n                ]),\n    }\n    self._inference_action = {\n        'world_vector':\n            tf.constant(0.5, shape=[self.inference_batch_size, 3]),\n        'rotation_delta':\n            tf.constant(0.5, shape=[self.inference_batch_size, 3]),\n        'terminate_episode':\n            tf.constant(\n                [0, 1] * self.inference_batch_size,\n                shape=[self.inference_batch_size, 2]),\n        'gripper_closedness_action':\n            tf.constant(0.5, shape=[self.inference_batch_size, 1]),"
        },
        {
            "comment": "This code is setting up a training action dictionary for the transformer network in a robotics application. It defines constants for 'world_vector', 'rotation_delta', 'terminate_episode', and 'gripper_closedness_action' within the self._train_action dictionary, with specified shapes and values. These values will be used to train the SequenceAgent, which uses a custom actor_network or default TransformerNetwork if none is provided.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":252-278",
            "content": "    }\n    self._train_action = {\n        'world_vector':\n            tf.constant(\n                0.5,\n                shape=[self.train_batch_size, self.time_sequence_length, 3]),\n        'rotation_delta':\n            tf.constant(\n                0.5,\n                shape=[self.train_batch_size, self.time_sequence_length, 3]),\n        'terminate_episode':\n            tf.constant(\n                [0, 1] * self.train_batch_size * self.time_sequence_length,\n                shape=[self.train_batch_size, self.time_sequence_length, 2]),\n        'gripper_closedness_action':\n            tf.constant(\n                0.5,\n                shape=[self.train_batch_size, self.time_sequence_length, 1]),\n    }\n  def _create_agent(self, actor_network=None):\n    \"\"\"Creates SequenceAgent using custom actor_network.\"\"\"\n    time_step_spec = ts.time_step_spec(observation_spec=self._state_spec)\n    if actor_network is None:\n      actor_network = transformer_network.TransformerNetwork\n    self._agent = sequence_agent.SequenceAgent("
        },
        {
            "comment": "Code creates a Transformer network and sets up for testing. It defines utility functions to handle observations, gets image values, and generates action logits. It also defines the specs needed for the test setup.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":279-305",
            "content": "        time_step_spec=time_step_spec,\n        action_spec=self._action_spec,\n        actor_network=actor_network,\n        actor_optimizer=tf.keras.optimizers.Adam(),\n        train_step_counter=tf.compat.v1.train.get_or_create_global_step(),\n        time_sequence_length=TIME_SEQUENCE_LENGTH)\n    self._num_action_tokens = (\n        # pylint:disable=protected-access\n        self._agent._actor_network._action_tokenizer._tokens_per_action)\n    # pylint:enable=protected-access\n  def setUp(self):\n    self._define_specs()\n    super().setUp()\n  def get_image_value(self, step_idx: int) -> float:\n    return float(step_idx) / self.time_sequence_length\n  def get_action_logits(self, batch_size: int, value: int,\n                        vocab_size: int) -> tf.Tensor:\n    return tf.broadcast_to(\n        tf.one_hot(value % vocab_size, vocab_size)[tf.newaxis, tf.newaxis, :],\n        [batch_size, 1, vocab_size])\n  def create_obs(self, value) -> dict[str, tf.Tensor]:\n    observations = {}\n    observations['image'] = value * self._inference_observation['image']"
        },
        {
            "comment": "This function takes a list of tokens, determines the batch size and token embedding size, reshapes the tokens to match the expected input format for the TransformerNetwork's _transformer() method, then returns the reshaped tokens. It also includes assertions for the vocabulary size and shape of the input tensor.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":306-334",
            "content": "    observations[\n        'natural_language_embedding'] = value * self._inference_observation[\n            'natural_language_embedding']\n    return observations\n  def fake_action_token_emb(self, action_tokens) -> tf.Tensor:\n    \"\"\"Just pad with zeros.\"\"\"\n    shape = action_tokens.shape\n    assert self.vocab_size > self.token_embedding_size\n    assert len(shape) == 4\n    return action_tokens[:, :, :, :self.token_embedding_size]\n  def fake_transformer(\n      self, all_tokens, training,\n      attention_mask) -> Union[tf.Tensor, Tuple[tf.Tensor, list[tf.Tensor]]]:\n    \"\"\"Fakes the call to TransformerNetwork._transformer.\"\"\"\n    del training\n    del attention_mask\n    # We expect ST00 ST01 A00 A01...\n    # Where:\n    # * ST01 is token 1 of state 0.\n    # * A01 is token 1 of action 0.\n    shape = all_tokens.shape.as_list()\n    batch_size = shape[0]\n    self.assertEqual(batch_size, 1)\n    emb_size = self.token_embedding_size\n    # transform to [batch_size, num_tokens, token_size]\n    all_tokens = tf.reshape(all_tokens, [batch_size, -1, emb_size])"
        },
        {
            "comment": "This code sets up a test environment for the transformer network, pads tokens to be of vocab_size, checks state/action alignment, and verifies if the index step is stored correctly. It uses tf.concat to concatenate zero-padded tokens, calculates action start index based on image and action token counts, and performs equality checks using get_image_value and tf.ones_like functions.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":335-358",
            "content": "    # Pads tokens to be of vocab_size.\n    self.assertGreater(self.vocab_size, self.token_embedding_size)\n    all_shape = all_tokens.shape\n    self.assertLen(all_shape.as_list(), 3)\n    output_tokens = tf.concat([\n        all_tokens,\n        tf.zeros([\n            all_shape[0], all_shape[1],\n            self.vocab_size - self.token_embedding_size\n        ])\n    ],\n                              axis=-1)\n    num_tokens_per_step = NUM_IMAGE_TOKENS + self._num_action_tokens\n    # Check state/action alignment.\n    window_range = min(self._step_idx + 1, self.time_sequence_length)\n    for j in range(window_range):\n      # The index step that is stored in j = 0.\n      first_step_idx = max(0, self._step_idx + 1 - self.time_sequence_length)\n      image_idx = j * num_tokens_per_step\n      action_start_index = image_idx + NUM_IMAGE_TOKENS\n      for t in range(NUM_IMAGE_TOKENS):\n        self.assertAllEqual(\n            self.get_image_value(first_step_idx + j) *\n            tf.ones_like(all_tokens[0][image_idx][:self.token_embedding_size]),"
        },
        {
            "comment": "This code tests the transformer network's output by comparing the logits of action dimensions against expected values. It asserts that the logits for certain actions match the expected token embeddings, ensuring correct prediction and understanding.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":359-378",
            "content": "            all_tokens[0][image_idx + t][:self.token_embedding_size])\n      # if j is not the current step in the window, all action dimensions\n      # from previous steps are already infered and thus can be checked.\n      action_dims_range = self.action_inf_idx if j == window_range - 1 else self._num_action_tokens\n      for t in range(action_dims_range):\n        token_idx = action_start_index + t\n        action_value = (first_step_idx + j) * self._num_action_tokens + t\n        self.assertAllEqual(\n            self.get_action_logits(\n                batch_size=batch_size,\n                value=action_value,\n                vocab_size=self.vocab_size)[0][0][:self.token_embedding_size],\n            all_tokens[0][token_idx][:self.token_embedding_size])\n    # Output the right action dimension value.\n    image_token_index = (\n        min(self._step_idx, self.time_sequence_length - 1) *\n        num_tokens_per_step)\n    transformer_shift = -1\n    action_index = (\n        image_token_index + NUM_IMAGE_TOKENS + self.action_inf_idx +"
        },
        {
            "comment": "This code is updating the output tokens by inserting action logits at a specific index, then concatenates the updated output tokens. The function also increments the action_inf_idx and initializes an empty list for attention scores. Finally, it returns the updated output tokens and the attention scores list.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network_test_set_up.py\":379-390",
            "content": "        transformer_shift)\n    action_value = self._step_idx * self._num_action_tokens + self.action_inf_idx\n    action_logits = self.get_action_logits(\n        batch_size=batch_size, value=action_value, vocab_size=self.vocab_size)\n    output_tokens = tf.concat([\n        output_tokens[:, :action_index, :], action_logits[:, :, :],\n        output_tokens[:, action_index + 1:, :]\n    ],\n                              axis=1)\n    self.action_inf_idx = (self.action_inf_idx + 1) % self._num_action_tokens\n    attention_scores = []\n    return output_tokens, attention_scores"
        }
    ]
}