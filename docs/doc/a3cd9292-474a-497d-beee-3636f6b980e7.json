{
    "summary": "This code introduces a transformer network for robotics tasks, incorporates training/inference functions, and computes action loss using transformer models. It logs network parameters, tracks input shapes, and visualizes spatial attention through TensorBoard. The code tokenizes images, handles data types, cropping if needed, with three main functions and helper functions.",
    "details": [
        {
            "comment": "The code is a Python file containing TensorFlow-based methods for sequence agents. It imports necessary libraries and modules, such as transformer, preprocessors, action_tokenizer, image_tokenizer, tensorspec_utils, and tf_agents.networks. This file likely includes functions related to creating and training sequence models using TensorFlow.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":0-26",
            "content": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tensorflow based methods for sequence agents.\"\"\"\nfrom typing import Optional, Tuple, Union, Any\nfrom absl import logging\nimport numpy as np\nfrom robotics_transformer import transformer\nfrom robotics_transformer.film_efficientnet import preprocessors\nfrom robotics_transformer.tokenizers import action_tokenizer\nfrom robotics_transformer.tokenizers import image_tokenizer\nfrom tensor2robot.utils import tensorspec_utils\nimport tensorflow as tf\nfrom tf_agents.networks import network"
        },
        {
            "comment": "This code is defining a class for a transformer-based actor network. It takes input tensor spec, output tensor spec, and other parameters such as vocab_size, token_embedding_size, num_layers, layer_size, num_heads, feed_forward_size, dropout_rate, time_sequence_length, crop_size, policy_info_spec, action_order, use_token_learner, return_attention_scores. This class extends the network.Network class from tf_agents.network module.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":27-57",
            "content": "from tf_agents.specs import tensor_spec\nfrom tf_agents.utils import nest_utils\nclass TransformerNetwork(network.Network):\n  \"\"\"A transformer based actor network.\"\"\"\n  def __init__(\n      self,\n      input_tensor_spec: tensorspec_utils.TensorSpecStruct,\n      output_tensor_spec: tensorspec_utils.TensorSpecStruct,\n      train_step_counter: int = 0,\n      vocab_size: int = 256,\n      token_embedding_size: int = 512,\n      num_layers: int = 1,\n      layer_size: int = 4096,\n      num_heads: int = 8,\n      feed_forward_size: int = 512,\n      dropout_rate: float = 0.1,\n      time_sequence_length: int = 1,\n      crop_size: int = 236,\n      policy_info_spec: Optional[dict[Any,\n                                      tensor_spec.BoundedTensorSpec]] = None,\n      action_order: Optional[list[str]] = None,\n      use_token_learner: Optional[bool] = True,\n      return_attention_scores: bool = False,\n      **kwargs):\n    \"\"\"Creates a transformer network.\n    Args:\n      input_tensor_spec: Nested list/tuple/dict of TensorSpecs, describing the"
        },
        {
            "comment": "This code defines a Transformer Network with specific input and output tensor shapes, train_step_counter for step counting, vocab_size for token dimensionality, embedding size, number of layers, layer sizes, heads, feed forward size, dropout rate, time sequence length, crop size, policy info specs, action order for the action tokenizer, and a flag to use a token learner.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":58-75",
            "content": "        shape of input tensor.\n      output_tensor_spec: Nested list/tuple/dict of TensorSpecs, describing the\n        shape of output tensor.\n      train_step_counter: Counter for number of steps.\n      vocab_size: Dimensionality of tokens from the output layer.\n      token_embedding_size: Dimensionality of tokens from the embedding layer.\n      num_layers: Number of transformer layers.\n      layer_size: Size of the multiple head attention layer.\n      num_heads: Number of heads for the multiple head attention layer.\n      feed_forward_size: Dimensionality of the feed_forward layer.\n      dropout_rate: Dropout rate.\n      time_sequence_length: Length of the time sequence.\n      crop_size: Height and width of the square crop, where original image will\n        be padded to allow full field of view to be extracted.\n      policy_info_spec: Spec on return value given return type of the return\n        tokenizer.\n      action_order: Order of actions for the action tokenizer.\n      use_token_learner: Whether to use token learner. See"
        },
        {
            "comment": "This code is initializing a Transformer network. It takes in various arguments like num_layers, layer_size, num_heads, feed_forward_size, dropout_rate, vocab_size, and return_attention_scores. It also sets up the image_tokenizer using the token_embedding_size and use_token_learner parameters. This network can be used for various tasks like language modeling or machine translation.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":76-102",
            "content": "        https://arxiv.org/abs/2106.11297\n      return_attention_scores: show attention scores in tensorboard.\n      **kwargs: Keyword parameter arguments.\n    \"\"\"\n    self._input_tensor_spec = input_tensor_spec\n    self._output_tensor_spec = output_tensor_spec\n    self._train_step_counter = train_step_counter\n    self._actions = None\n    self._returns = None\n    self._vocab_size = vocab_size\n    self._token_embedding_size = token_embedding_size\n    self._time_sequence_length = time_sequence_length\n    self._crop_size = crop_size\n    self._transformer = transformer.Transformer(\n        num_layers=num_layers,\n        layer_size=layer_size,\n        num_heads=num_heads,\n        feed_forward_size=feed_forward_size,\n        dropout_rate=dropout_rate,\n        vocab_size=self._vocab_size,\n        return_attention_scores=return_attention_scores)\n    # create tokenizers\n    self._image_tokenizer = image_tokenizer.RT1ImageTokenizer(\n        embedding_output_dim=self._token_embedding_size,\n        use_token_learner=use_token_learner)"
        },
        {
            "comment": "This code is initializing a TransformerNetwork object, setting various attributes including the action and image tokenizers, generating masks, defining loss functions, and specifying input tensor specifications.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":103-128",
            "content": "    self._action_tokenizer = action_tokenizer.RT1ActionTokenizer(\n        output_tensor_spec,\n        vocab_size=self._vocab_size,\n        action_order=action_order)\n    self._tokens_per_action = self._action_tokenizer.tokens_per_action\n    self._tokens_per_context_image = self._image_tokenizer.tokens_per_context_image\n    # generate loss and attention masks\n    self._generate_masks()\n    # define mappings to token embedding size\n    self._action_token_emb = tf.keras.layers.Dense(self._token_embedding_size)\n    # define loss function\n    self._loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n    self._attention_scores = []\n    self._use_token_learner = use_token_learner\n    super(TransformerNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec, **kwargs)\n    self._state_spec = {\n        # Force this to be 4 dimension due to b/254902773.\n        # Otherwise can be dimension 3.\n        'context_image_tokens':\n            tensor_spec.TensorSpec("
        },
        {
            "comment": "This code defines a Transformer network for robotics tasks, with inputs like context image tokens, action tokens, and sequence index. It also includes a property for attention scores to aid in debugging and visualization. The context image tokens have a shape of (time_sequence_length, tokens_per_context_image, 1, token_embedding_size) and are of type tf.float32. The action tokens have a shape of (time_sequence_length, tokens_per_action, 1, 1) and are of type tf.int32. The sequence index is used to keep track of the current position within the time sequence and has a shape of (1, 1, 1, 1) and dtype tf.int32.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":129-153",
            "content": "                shape=(time_sequence_length, self._tokens_per_context_image, 1,\n                       token_embedding_size),\n                dtype=tf.float32,\n                name='context_image_tokens'),\n        'action_tokens':\n            tensor_spec.TensorSpec(\n                shape=(time_sequence_length, self._tokens_per_action, 1, 1),\n                dtype=tf.int32,\n                name='action_tokens'),\n        # Stores where in the window we are.\n        # This value is within range [0, time_sequence_length + 1].\n        # When seq_idx == time_sequence_length, context_image_tokens and\n        # action_tokens need to be shifted to the left.\n        'seq_idx':\n            tensor_spec.TensorSpec(\n                shape=(1, 1, 1, 1), dtype=tf.int32, name='seq_idx')\n    }\n  @property\n  def attention_scores(self) -> list[tf.Tensor]:\n    \"\"\"Return attention score. This is for debugging/visualization purpose.\"\"\"\n    return self._attention_scores\n  def _get_action_index_for_token(self, k):\n    \"\"\"Returns action associated with the token at given position `k`."
        },
        {
            "comment": "This code defines two methods for a class, one to determine the action index given a position in the sequence and another to generate masks for action prediction loss and attention visualization. The first method returns -1 if k is not within the valid range or is part of an image token; otherwise, it returns the index of the action that the position belongs to. The second method calculates the total number of tokens in a full sequence by multiplying the time step length with the single time step number of tokens (which includes both action and context image tokens).",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":155-183",
            "content": "    If k is not an action token then it returns -1.\n    If k is part of the first action in the sequence then returns 0 etc.\n    Args:\n        k: an int that represents the position in the sequence.\n    Returns:\n        The index of the action that this position belongs to, or if this\n        position is part of an image token then returns -1.\n    \"\"\"\n    if (k < 0 or k >= self._all_num_tokens):\n      return -1\n    n = k\n    if n % self._single_time_step_num_tokens < self._tokens_per_context_image:\n      return -1\n    return int(n / self._single_time_step_num_tokens)\n  def _generate_masks(self):\n    \"\"\"Generate mask for action prediction loss and attention visualization.\"\"\"\n    # each time step = [image, action]\n    self._single_time_step_num_tokens = (\n        self._tokens_per_action + self._tokens_per_context_image)\n    # full sequence = [prefix context + N x timestep + postfix context]\n    self._all_num_tokens = (\n        self._time_sequence_length * self._single_time_step_num_tokens)\n    # create mask for action predition loss"
        },
        {
            "comment": "The code defines a transformer network for robotics, initializing action tokens mask and default attention mask. The action tokens mask is created based on the number of tokens per action and tokens per context image, and is converted to a tf.constant. The look ahead mask ensures causality by using tf.linalg.band_part to set the upper triangle to zeros. The code also defines an action_mask which ignores actions of previous steps, only allowing current and future actions.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":184-206",
            "content": "    self._action_tokens_mask = []\n    for n in range(0, self._all_num_tokens, self._single_time_step_num_tokens):\n      for x in range(0, self._tokens_per_action, 1):\n        self._action_tokens_mask.append(x + n + self._tokens_per_context_image)\n    self._action_tokens_mask = tf.constant(\n        self._action_tokens_mask, dtype=tf.int32)\n    # The look ahead mask ensures causality.\n    self._default_attention_mask = tf.linalg.band_part(\n        tf.ones((self._all_num_tokens, self._all_num_tokens)), -1, 0)\n    action_mask = np.ndarray(\n        shape=(self._all_num_tokens, self._all_num_tokens), dtype=int)\n    for i in range(self._all_num_tokens):\n      for j in range(self._all_num_tokens):\n        action_i = self._get_action_index_for_token(i)\n        action_j = self._get_action_index_for_token(j)\n        mask = 0\n        if action_i != -1 and action_j != -1:\n          # Ignore actions of previous steps.\n          if action_j < action_i:\n            mask = 1\n          # If we're not auto-regression, ignore action dimensions of current"
        },
        {
            "comment": "The code snippet defines a function that calls the transformer. It takes context and action tokens as input, along with batch size, training mode, and an optional attention mask. It returns output tokens and optionally the attention scores. The attention mask is used to mask the transformer's attention for certain positions based on action sequences.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":207-236",
            "content": "          # step.\n          if (action_j == action_i and j <= i):\n            mask = 1\n        action_mask[i, j] = mask\n    self._default_attention_mask -= action_mask\n  def _transformer_call(\n      self,\n      context_image_tokens: tf.Tensor,\n      action_tokens: tf.Tensor,\n      batch_size: int,\n      training: bool,\n      attention_mask: tf.Tensor,\n  ) -> Union[tf.Tensor, Tuple[tf.Tensor, tf.Tensor]]:\n    \"\"\"Calls the transformer.\n    Args:\n      context_image_tokens: Tokenized context and image in Tensor of shape `(B,\n        T, num token, -1)`.\n      action_tokens: Discrete action token sequence of size [8, 256].\n      batch_size: Batch size as when reshaping all tokens.\n      training: Whether to run the transformer in training mode.\n      attention_mask: Optional bool tensor for masking transformer's attention.\n    Returns:\n      Output tokens in Tensor of shape `(B, T, dim)`. If\n      return_attention_scores, also return the attention scores of\n      shape `(B, T, dim)`.\n    \"\"\"\n    input_token_sequence = self._assemble_input_token_sequence("
        },
        {
            "comment": "This code defines a transformer network that takes in image tokens, action tokens, and attention mask as input. The network then performs the transformer operation to generate output tokens. The `_get_tokens_and_mask` function tokenizes all inputs and generates the attention mask, while the `_transformer_call_and_slice` function handles calling the transformer with optional slicing of the results.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":237-262",
            "content": "        context_image_tokens, action_tokens, batch_size)\n    # run transformer\n    output_tokens, self._attention_scores = self._transformer(\n        input_token_sequence, training, attention_mask)\n    return output_tokens\n  def _get_tokens_and_mask(self,\n                           observations: dict[str, tf.Tensor],\n                           network_state: dict[str, tf.Tensor],\n                           training: bool = False):\n    # tokenize all inputs\n    context_image_tokens, network_state = self._tokenize_images(\n        observations, network_state, training)\n    action_tokens = self._tokenize_actions(observations, network_state)\n    # generate transformer attention mask\n    attention_mask = self._default_attention_mask\n    return (context_image_tokens, action_tokens, attention_mask)\n  def _transformer_call_and_slice(self,\n                                  *args,\n                                  slice_start: int = 0,\n                                  slice_length: int = 1,\n                                  **kwargs) -> Tuple[tf.Tensor, tf.Tensor]:"
        },
        {
            "comment": "This code defines a transformer network for processing observations (image and natural language) to output detokenized actions. The `_transformer_call` function is called with provided arguments, and the resultant token logits are obtained using slicing. The `call` method takes observation data, network state, and training flag as inputs. It determines the outer rank to call the transformer network in either training or inference mode. The outputs are detokenized output actions and network state.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":263-290",
            "content": "    output_tokens = self._transformer_call(*args, **kwargs)\n    slice_end = slice_start + slice_length\n    token_logits = output_tokens[:, slice_start:slice_end, :]\n    token = tf.argmax(token_logits, axis=-1, output_type=tf.int32)\n    return token, token_logits\n  def call(self,\n           observations: dict[str, tf.Tensor],\n           network_state: dict[str, tf.Tensor],\n           training: bool = False):\n    \"\"\"Calls the transformer network.\n    Args:\n      observations: Observation data including image and natural language\n        embedding in dict of Tensors.\n      network_state: Network state data including time step, image, action\n        tokens, step number in dict of Tensors.\n      training: Whether to call transformer network in training mode.\n    Returns:\n      A tuple `(Detokenized output actions, network state)`.\n    \"\"\"\n    # used to determine training vs inference call\n    # outer_rank will be 2 -> [b, t] during training and\n    # outer_rank will be 1 -> [b] during inference\n    outer_rank = self._get_outer_rank(observations)"
        },
        {
            "comment": "Inference process begins when outer_rank equals 1, and the code initializes variables to iterate through a loop and predict action tokens one by one. The transformer network is shifted by default, predicting next token in training tasks. It gets action predicted at time_step, starting from a specific index.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":291-313",
            "content": "    assert outer_rank in (1, 2)\n    b, t = self._get_batch_size_and_seq_len(network_state)\n    context_image_tokens, action_tokens, attention_mask = self._get_tokens_and_mask(\n        observations, network_state, training)\n    self._aux_info = {'action_labels': action_tokens}\n    if outer_rank == 1:  # This is an inference call\n      # run transformer in loop to produce action tokens one-by-one\n      seq_idx = tf.reshape(network_state['seq_idx'], [1])[0]\n      action_t = tf.minimum(seq_idx, self._time_sequence_length - 1)\n      # Transformer shifts all to the left by one step by default (it's usually\n      # predicting the next token as default training task...).\n      transformer_shift = -1\n      # We only want to get the action predicted at time_step.\n      start_index = (\n          transformer_shift + self._tokens_per_context_image + action_t *\n          (self._single_time_step_num_tokens))\n      current_action_tokens = []\n      action_predictions_logits = []\n      for k in range(self._tokens_per_action):"
        },
        {
            "comment": "The code is slicing and concatenating action tokens to represent the entire sequence of actions. It appends the logits and tokens for each action, reshapes the action tokens, and updates the auxiliary information dictionary.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":314-336",
            "content": "        action_index = start_index + k\n        token, token_logits = self._transformer_call_and_slice(\n            context_image_tokens,\n            action_tokens,\n            attention_mask=attention_mask,\n            batch_size=b,\n            training=training,\n            slice_start=action_index  # slicing single action dimension\n        )\n        action_predictions_logits.append(token_logits)\n        current_action_tokens.append(token)\n        # action_tokens is [b, t * self._tokens_per_action]\n        action_tokens = tf.reshape(action_tokens, [b, -1])\n        action_start_index = (action_t * self._tokens_per_action) + k\n        action_tokens = tf.concat([\n            action_tokens[:, :action_start_index], token,\n            action_tokens[:, action_start_index + 1:]\n        ],\n                                  axis=1)\n        # action_tokens is [b, t, self._tokens_per_action]\n        action_tokens = tf.reshape(action_tokens,\n                                   [b, t, self._tokens_per_action])\n      self._aux_info.update({"
        },
        {
            "comment": "This code is part of a transformer network for robotics. It concatenates action predictions logits, predicted tokens for output, and state_action_tokens to form the next network_state. The time_step is incremented for the next inference call.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":337-355",
            "content": "          # action_predictions_logits is\n          # [b, self._tokens_per_action, self._vocab_size]\n          'action_predictions_logits': tf.concat(action_predictions_logits, 1)\n      })\n      # predicted_tokens_for_output is [b, self._tokens_per_action]\n      predicted_tokens_for_output = tf.concat(current_action_tokens, 1)\n      # state_action_tokens is [b, 1, self._tokens_per_action, 1, 1]\n      one_state_action_tokens = predicted_tokens_for_output[:, tf.newaxis, :,\n                                                            tf.newaxis,\n                                                            tf.newaxis]\n      state_action_tokens = network_state['action_tokens']\n      network_state['action_tokens'] = tf.concat([\n          state_action_tokens[:, :action_t, ...], one_state_action_tokens,\n          state_action_tokens[:, action_t + 1:, ...]\n      ],\n                                                 axis=1)\n      # Increment the time_step for the next inference call.\n      network_state['seq_idx'] = tf.reshape("
        },
        {
            "comment": "In this code segment, a transformer forward pass is performed for training, and the predicted actions are gathered. The action logits are reshaped, with only the last action taken as the final output. Predicted tokens for output are extracted from the action logits using argmax function, resulting in predicted_tokens_for_output (b, self._tokens_per_action).",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":356-382",
            "content": "          tf.minimum(seq_idx + 1, self._time_sequence_length), [-1, 1, 1, 1, 1])\n      self._loss = tf.constant(0.0)\n    else:\n      # training call --> simply run one transformer forward pass\n      output_tokens = self._transformer_call(\n          context_image_tokens,\n          action_tokens,\n          attention_mask=attention_mask,\n          batch_size=b,\n          training=training)\n      # Gather all predicted actions for the action loss.\n      action_logits = tf.gather(\n          output_tokens, self._action_tokens_mask - 1, axis=1)\n      action_logits_for_training = tf.reshape(\n          action_logits, [b, t, self._tokens_per_action, -1])\n      # Only take the last action as the action.\n      # action_logits_for_output is [b, self._tokens_per_action, emb]\n      action_logits_for_output = action_logits_for_training[:, -1]\n      # predicted_tokens_for_output is [b, self._tokens_per_action]\n      predicted_tokens_for_output = tf.argmax(\n          action_logits_for_output, axis=-1, output_type=tf.int32)\n      num_items = ("
        },
        {
            "comment": "This code appears to be part of a transformer network model for robotics. It calculates the action loss and stores predictions and auxiliary information for visualization purposes. The `add_summaries` function adds summaries to observation data including image and natural language input. This code likely forms part of a larger training or prediction pipeline for a robotics system using transformer models.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":383-412",
            "content": "          tf.cast(b * t, tf.float32) * self._single_time_step_num_tokens)\n      action_loss = tf.reduce_mean(\n          self._loss_object(action_tokens, action_logits_for_training) /\n          num_items,\n          axis=-1)\n      self._loss = action_loss\n      # store action labels and predictions for visualization\n      self._aux_info.update({\n          'action_predictions':\n              tf.argmax(\n                  action_logits_for_training, axis=-1, output_type=tf.int32),\n          'action_loss':\n              action_loss,\n          'actor_loss_mask':\n              tf.ones([b], dtype=tf.float32)\n      })\n    output_actions = self._action_tokenizer.detokenize(\n        predicted_tokens_for_output)\n    return output_actions, network_state\n  def add_summaries(self, observations: dict[str, tf.Tensor],\n                    logging_info: dict[str, tf.Tensor], debug_summaries: bool,\n                    training: bool) -> None:\n    \"\"\"Adds summaries.\n    Args:\n      observations: Observation data including image and natural language"
        },
        {
            "comment": "This code calculates the number of trainable parameters in the transformer network and logs this information using TensorFlow's summary scalar. Additionally, it prepares an image for logging by concatenating images from different timesteps and batches. This helps track the shape and size of the input during training or inference passes. The code also handles TPUs and non-TPU workers separately by including debug summaries if needed.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":413-436",
            "content": "        instruction in dict of Tensors.\n      logging_info: Dict with all data stored for logging during training pass.\n      debug_summaries: Whether to include debug summaries.\n      training: Whether this function is called during training or inference.\n    \"\"\"\n    num_params = 0\n    for weight in self.trainable_weights:\n      weight_params = 1\n      for dim in weight.shape:\n        weight_params *= dim\n      num_params += weight_params\n    tf.compat.v2.summary.scalar(name='num_params', data=num_params)\n    # debug_summaries are for the non-tpu worker, train_summary.\n    if debug_summaries:\n      image = observations['image']  # [b, t, h, w, c]\n      image_h = image.shape[2]\n      image_w = image.shape[3]\n      batch_size = image.shape[0]\n      num_ts = image.shape[1]\n      logging.info('image shape %s', image.shape)\n      # Concat images for different timesteps across width.\n      image = tf.concat(tf.unstack(image, axis=1), 2)\n      # Concat images for different batches (up to 8) across height.\n      image = tf.expand_dims(tf.concat(tf.unstack(image, axis=0)[0:8], 0), 0)"
        },
        {
            "comment": "This code snippet is part of a transformer network. It logs image and text data for training, and if attention scores are enabled, it also logs pairwise attention scores. The logging is done using TensorFlow's summary functions to store the information as summaries. These summaries will be used by TensorBoard, a visualization tool provided by TensorFlow, to help in monitoring and analyzing the model during training.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":437-458",
            "content": "      tf.summary.image(\n          'observations/image',\n          image,\n          step=self._train_step_counter,\n          # Single output since we have concatenated images along batch.\n          max_outputs=1)\n      # [b, t], strings\n      if 'natural_language_instruction' in observations:\n        task = observations['natural_language_instruction'][:, 0]\n        tf.summary.text(\n            'natural_language_instruction', task, step=self._train_step_counter)\n      if self.attention_scores and not self._use_token_learner:\n        for l_idx, layer_attention_score in enumerate(self.attention_scores):\n          logging.info('Attention score shape: %s, %s', l_idx,\n                       layer_attention_score.shape)\n          for head_idx in range(layer_attention_score.shape[1]):\n            pairwise_attention = tf.expand_dims(\n                layer_attention_score[:, head_idx], -1)\n            # pairwise attention shape (16, 552, 552, 1)\n            # make attention from different time steps comparable\n            pairwise_attention = pairwise_attention * np.arange("
        },
        {
            "comment": "This code calculates the spatial attention for a transformer network and visualizes it, supporting a specific pipeline configuration. The spatial attention is calculated by summing pairwise attention values and normalizing them. Then, the result is resized and concatenated to form an image-like output. This process requires specific dimensions and parameters for visualization.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":459-478",
            "content": "                1, pairwise_attention.shape[1] + 1)[None, :, None, None]\n            # visualize spatial attention, note this only supports\n            # mk1_500tasks_transformer pipeline with no token learner\n            img_tf_ts = tf.reshape(\n                tf.transpose(\n                    tf.reshape(\n                        tf.reduce_sum(pairwise_attention, axis=1) / np.arange(\n                            pairwise_attention.shape[1], 0, -1)[None, :, None],\n                        [batch_size, num_ts, -1]),\n                    [0, 2, 1])[:, :-self._tokens_per_action, :],\n                [-1, 9, 9, num_ts])\n            img_tf_ts = tf.image.resize(\n                img_tf_ts, [image_h, image_w],\n                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n            img_tf_ts_concat = tf.concat(tf.unstack(img_tf_ts, axis=3), 2)\n            img_tf_ts_concat_min = tf.reduce_min(\n                img_tf_ts_concat, axis=[1, 2], keepdims=True)\n            img_tf_ts_concat = (img_tf_ts_concat - img_tf_ts_concat_min) / ("
        },
        {
            "comment": "This code segment is part of a transformer network. It calculates attention scores for each head in a layer and visualizes them as images. It concatenates input sequences, subtracts minimum value from maximum, and reshapes the output. The resulting image is then overlaid on the original image using element-wise multiplication, and both are visualized in separate summaries.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":479-499",
            "content": "                tf.reduce_max(img_tf_ts_concat, axis=[1, 2], keepdims=True) -\n                img_tf_ts_concat_min)\n            img_tf_ts_concat = tf.concat(\n                tf.unstack(img_tf_ts_concat, axis=0)[:8], 0)\n            img_tf_ts_concat = tf.expand_dims(\n                tf.expand_dims(img_tf_ts_concat, 0), -1)\n            tf.summary.image(\n                'attention/layer_{}/head_{}'.format(l_idx, head_idx),\n                img_tf_ts_concat,\n                step=self._train_step_counter,\n                # Single output since we have concatenated images along batch.\n                max_outputs=1)\n            if img_tf_ts_concat.shape[1] == image.shape[\n                1] and img_tf_ts_concat.shape[2] == image.shape[2]:\n              # can overlay\n              overlay_viz = tf.cast(\n                  (tf.cast(image, tf.float32) * (0.2 + img_tf_ts_concat) / 1.2),\n                  tf.uint8)\n              tf.summary.image(\n                  'overlay_attention/layer_{}/head_{}'.format(l_idx, head_idx),"
        },
        {
            "comment": "This code logs action information for a transformer network. It calculates the accuracy of action predictions against action labels, and logs the accuracy using TensorFlow's summary scalar function. The accuracy is averaged across time steps to provide a comprehensive evaluation of the model's performance.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":500-520",
            "content": "                  overlay_viz,\n                  step=self._train_step_counter,\n                  # Single output since we have concatenated images along batch.\n                  max_outputs=1)\n    # log action info\n    action_labels = tf.boolean_mask(logging_info['action_labels'],\n                                    logging_info['actor_loss_mask'])\n    action_predictions = tf.boolean_mask(logging_info['action_predictions'],\n                                         logging_info['actor_loss_mask'])\n    with tf.name_scope('ActionTokens'):\n      token_accuracy = (\n          tf.cast(tf.equal(action_labels, action_predictions), tf.float32))\n      accuracy = tf.reduce_mean(token_accuracy)\n      tf.compat.v2.summary.scalar(\n          name='accuracy', data=accuracy, step=self._train_step_counter)\n      # Accuracy across timesteps\n      for t in range(self._time_sequence_length):\n        tf.compat.v2.summary.scalar(\n            name='accuracy/time_step/{}'.format(t),\n            data=tf.reduce_mean(token_accuracy[:, t, :]),"
        },
        {
            "comment": "This code calculates the token accuracy and loss for each action token in the model, then logs these values using TensorFlow's summary function. It iterates through the available actions, handling both integer32 and other data types separately. For each token, it logs histograms of labels and predictions with their corresponding step number.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":521-547",
            "content": "            step=self._train_step_counter)\n      token_index = 0\n      for k in self._action_tokenizer.action_order:\n        spec = self._action_tokenizer.action_spec[k]\n        if spec.dtype == tf.int32:\n          n_tokens = 1\n        else:\n          n_tokens = spec.shape[0]\n        action_token_accuracy = tf.reduce_mean(\n            token_accuracy[:, :, token_index:token_index + n_tokens])\n        tf.compat.v2.summary.scalar(\n            name='accuracy/action_type/{}'.format(k),\n            data=action_token_accuracy,\n            step=self._train_step_counter)\n        for n in range(n_tokens):\n          tf.summary.histogram(\n              'tokens/{}_{}/labels'.format(k, n + 1),\n              action_labels[:, :, token_index],\n              step=self._train_step_counter)\n          tf.summary.histogram(\n              'tokens/{}_{}/predictions'.format(k, n + 1),\n              action_predictions[:, :, token_index],\n              step=self._train_step_counter)\n          token_index += 1\n    # log loss components\n    with tf.name_scope('TokenLosses'):"
        },
        {
            "comment": "This function tokenizes images for both training and inference calls. It reshapes the image to match the input shape required by the model, extracts context from observation, preprocesses the image by converting its data type and cropping it if necessary using a random seed.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":548-574",
            "content": "      tf.compat.v2.summary.scalar(\n          name='action_loss',\n          data=tf.reduce_mean(logging_info['action_loss']),\n          step=self._train_step_counter)\n  def _tokenize_images(self, observations, network_state, training):\n    image = observations['image']  # [b, t, h, w, c]\n    outer_rank = self._get_outer_rank(observations)\n    if outer_rank == 1:  # This is an inference call\n      seq_idx = tf.reshape(network_state['seq_idx'], [1])[0]\n      time_step = tf.minimum(seq_idx, self._time_sequence_length - 1)\n      image = tf.expand_dims(image, 1)\n    image_shape = tf.shape(image)\n    b = image_shape[0]\n    input_t = image_shape[1]\n    h = image_shape[2]\n    w = image_shape[3]\n    c = image_shape[4]\n    context = self._extract_context_from_observation(observations, input_t)\n    image = tf.reshape(image, [b * input_t, h, w, c])\n    seed = tf.random.uniform(shape=(2,), maxval=2**30, dtype=tf.int32)\n    image = preprocessors.convert_dtype_and_crop_images(\n        image,\n        crop_size=self._crop_size,"
        },
        {
            "comment": "This code reshapes the image and context_image_tokens, prepares them for input to the transformer network, and handles inference by shifting images to align with time dim. This ensures that each call to the transformer network receives appropriate inputs.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":575-596",
            "content": "        training=training,\n        pad_then_crop=True,\n        convert_dtype=True,\n        seed=seed)\n    image = tf.reshape(image, [b, input_t, h, w, c])\n    context_image_tokens = self._image_tokenizer(\n        image, context=context, training=training)\n    num_tokens = tf.shape(context_image_tokens)[2]\n    context_image_tokens = tf.reshape(context_image_tokens,\n                                      [b, input_t, num_tokens, 1, -1])\n    if outer_rank == 1:  # This is an inference call\n      network_state['context_image_tokens'] = tf.reshape(\n          network_state['context_image_tokens'], [\n              b, self._time_sequence_length, self._tokens_per_context_image, 1,\n              -1\n          ])\n      state_image_tokens = network_state['context_image_tokens']\n      # network_state as input for this call is the output from the last call.\n      # Therefore, we need to shift all images to the left by 1 in the time axis\n      # to align w/ the time dim in this call.\n      state_image_tokens = tf.cond(\n          seq_idx == self._time_sequence_length,"
        },
        {
            "comment": "Code snippet performs image tokenization, concatenates tokens for context and state images, updates network state, and tokenizes actions for inference calls. It shifts all actions by 1 to the left if the sequence index equals the time sequence length.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":597-617",
            "content": "          lambda: tf.roll(state_image_tokens, -1, axis=1),\n          lambda: state_image_tokens)\n      context_image_tokens = tf.concat([\n          state_image_tokens[:, :time_step, ...], context_image_tokens,\n          state_image_tokens[:, time_step + 1:, ...]\n      ],\n                                       axis=1)\n      network_state['context_image_tokens'] = context_image_tokens\n    return context_image_tokens, network_state\n  def _tokenize_actions(self, observations, network_state):\n    outer_rank = self._get_outer_rank(observations)\n    if outer_rank == 1:  # This is an inference call\n      action_tokens = tf.squeeze(network_state['action_tokens'], [3, 4])\n      seq_idx = tf.reshape(network_state['seq_idx'], [1])[0]\n      # network_state as input for this call is the output from the last call.\n      # Therefore, we need to shift all actions by 1 to the left.\n      action_tokens = tf.cond(seq_idx == self._time_sequence_length,\n                              lambda: tf.roll(action_tokens, -1, axis=1),"
        },
        {
            "comment": "This code defines a method that retrieves action tokens and assembles them into the input token sequence. It first checks if outer_rank is 2, then gets the batch size and sequence length from the network state or assigns zeros to action tokens. If actions are not None, it tokenizes them using _action_tokenizer. Then, it embeds the action tokens, pads them with zeros, expands dimensions, concatenates context image tokens, and reshapes the result into the input token sequence.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":618-643",
            "content": "                              lambda: action_tokens)\n    else:\n      assert outer_rank == 2\n      if self._actions is None:\n        b, t = self._get_batch_size_and_seq_len(network_state)\n        action_tokens = tf.zeros(\n            shape=[b, t, self._tokens_per_action], dtype=tf.int32)\n      else:\n        action_tokens = self._action_tokenizer.tokenize(self._actions)\n    return action_tokens\n  def _assemble_input_token_sequence(self, context_image_tokens, action_tokens,\n                                     batch_size):\n    # embed action tokens\n    action_tokens = tf.one_hot(action_tokens, self._vocab_size)\n    action_tokens = self._action_token_emb(action_tokens)\n    action_tokens = tf.zeros_like(action_tokens)  # b/260260205\n    # Because of b/254902773, we need to add 1 extra dimension.\n    action_tokens = tf.expand_dims(action_tokens, axis=-2)\n    # assemble token sequence\n    input_token_sequence = tf.concat([context_image_tokens, action_tokens],\n                                     axis=2)\n    input_token_sequence = tf.reshape("
        },
        {
            "comment": "This code contains three functions: `_get_input_token_sequence`, `_extract_context_from_observation`, and `set_actions`. The first function, `_get_input_token_sequence`, returns the input token sequence. The second function, `_extract_context_from_observation`, extracts context from observations. Lastly, the `set_actions` function sets actions that will be tokenized and used in the transformer network. The code also contains a helper function, `_get_outer_rank`, which is used to determine if the call is for training or inference.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":644-668",
            "content": "        input_token_sequence, [batch_size, -1, self._token_embedding_size])\n    return input_token_sequence\n  def _extract_context_from_observation(self, observations, seq_len):\n    \"\"\"Extract context from observation.\"\"\"\n    context = None\n    if 'natural_language_embedding' in observations:\n      outer_rank = self._get_outer_rank(observations)\n      context = observations['natural_language_embedding']  # [b, t, emb-size]\n      if outer_rank == 1:\n        context = tf.tile(context[:, None], [1, seq_len, 1])\n    return context\n  def set_actions(self, actions: tensorspec_utils.TensorSpecStruct):\n    \"\"\"Sets actions that will be tokenized and used in transformer network.\n    Args:\n      actions: actions to be tokenized and used in transformer network. example\n        actions are terminate = [0, 1] world_vector = [0.9, 0.8, -0.3]\n        rotation_delta = [-0.1, 0.2, .6] gripper_closedness = 0.9\n    \"\"\"\n    self._actions = actions\n  def _get_outer_rank(self, observations):\n    # used to determine training vs inference call"
        },
        {
            "comment": "The code contains three functions: 1) `get_outer_rank` returns the outer dimension rank of a tensor based on its shape and the input tensor specification, useful for batch sizes during training and inference. 2) `_get_batch_size_and_seq_len` extracts the batch size and sequence length from the image shape of the network state, which can be used to analyze data dimensions. 3) `get_actor_loss` and `get_aux_info` are getter methods for the actor loss and auxiliary information, respectively, in a transformer network context.",
            "location": "\"/media/root/Prima/works/robotics_transformer/docs/src/transformer_network.py\":669-683",
            "content": "    # outer_rank will be 2 -> [b, t] during training and\n    # outer_rank will be 1 -> [b] during inference\n    return nest_utils.get_outer_rank(observations, self._input_tensor_spec)\n  def _get_batch_size_and_seq_len(self, network_state):\n    image_shape = tf.shape(network_state['context_image_tokens'])\n    b = image_shape[0]\n    t = image_shape[1]\n    return b, t\n  def get_actor_loss(self) -> tf.Tensor:\n    return self._loss\n  def get_aux_info(self) -> dict[str, Any]:\n    return self._aux_info"
        }
    ]
}